# Interactive RAG Learning System
# Designed for interview preparation - –ø–æ–Ω—è—Ç–Ω–æ –æ–±—è—Å–Ω–µ–Ω–∏–µ –Ω–∞ –≤—Å—è–∫–∞ —Å—Ç—ä–ø–∫–∞

import os
import json
from typing import List, Dict, Any
from datetime import datetime

# LangChain imports
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain.schema import BaseRetriever

# For similarity calculations
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity


class InteractiveRAGSystem:
    """
    Interactive RAG System –∑–∞ —É—á–µ–±–Ω–∏ —Ü–µ–ª–∏

    –ü–æ–∫–∞–∑–≤–∞ –∫–∞–∫ —Ä–∞–±–æ—Ç–∏ –≤—Å—è–∫–∞ —Å—Ç—ä–ø–∫–∞:
    1. Document Processing (–∫–∞–∫ —Å–µ –æ–±—Ä–∞–±–æ—Ç–≤–∞—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ç–µ)
    2. Embedding Creation (–∫–∞–∫ —Å–µ —Å—ä–∑–¥–∞–≤–∞—Ç –≤–µ–∫—Ç–æ—Ä–∏—Ç–µ)
    3. Vector Storage (–∫–∞–∫ —Å–µ —Å—ä—Ö—Ä–∞–Ω—è–≤–∞—Ç –≤ –±–∞–∑–∞ –¥–∞–Ω–Ω–∏)
    4. Similarity Search (–∫–∞–∫ —Å–µ —Ç—ä—Ä—Å–∏ —Å—Ö–æ–¥—Å—Ç–≤–æ)
    5. Context Augmentation (–∫–∞–∫ —Å–µ –¥–æ–±–∞–≤—è –∫–æ–Ω—Ç–µ–∫—Å—Ç)
    6. Response Generation (–∫–∞–∫ —Å–µ –≥–µ–Ω–µ—Ä–∏—Ä–∞ –æ—Ç–≥–æ–≤–æ—Ä)
    """

    def __init__(self, verbose=True):
        self.verbose = verbose
        self.documents = []
        self.document_chunks = []
        self.embeddings_model = None
        self.vectorstore = None

        if self.verbose:
            print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–Ω–µ –Ω–∞ RAG —Å–∏—Å—Ç–µ–º–∞...")
            print("=" * 60)

    def step1_setup_embeddings(self):
        """
        –°–¢–™–ü–ö–ê 1: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ embedding –º–æ–¥–µ–ª

        Embeddings —Å–∞ –Ω–∞—á–∏–Ω—ä—Ç –¥–∞ –ø—Ä–µ–≤—Ä—ä—â–∞–º–µ —Ç–µ–∫—Å—Ç –≤ —á–∏—Å–ª–∞ (–≤–µ–∫—Ç–æ—Ä–∏),
        –∑–∞ –¥–∞ –º–æ–∂–µ –∫–æ–º–ø—é—Ç—ä—Ä—ä—Ç –¥–∞ —Ä–∞–∑–±–∏—Ä–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–∞—Ç–∞
        """
        if self.verbose:
            print("üìä –°–¢–™–ü–ö–ê 1: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ Embedding –º–æ–¥–µ–ª")
            print("-" * 40)
            print("Embedding –º–æ–¥–µ–ª—ä—Ç –ø—Ä–µ–≤—Ä—ä—â–∞ —Ç–µ–∫—Å—Ç –≤ —á–∏—Å–ª–æ–≤–∏ –≤–µ–∫—Ç–æ—Ä–∏")
            print("–ò–∑–ø–æ–ª–∑–≤–∞–º–µ 'sentence-transformers/all-MiniLM-L6-v2' - –º–∞–ª—ä–∫ –Ω–æ –µ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –º–æ–¥–µ–ª")

        # –ú–∞–ª—ä–∫ –∏ –±—ä—Ä–∑ embedding –º–æ–¥–µ–ª
        self.embeddings_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},  # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ CPU –∑–∞ –ø–æ-–±—ä—Ä–∑–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
            encode_kwargs={'normalize_embeddings': False}
        )

        if self.verbose:
            print("‚úÖ Embedding –º–æ–¥–µ–ª –≥–æ—Ç–æ–≤!")
            print()

    def step2_load_documents(self, texts: List[str]):
        """
        –°–¢–™–ü–ö–ê 2: –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∏

        –¢—É–∫ —Ä–∞–∑–¥–µ–ª—è–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ç–µ –Ω–∞ –ø–æ-–º–∞–ª–∫–∏ —á–∞—Å—Ç–∏ (chunks),
        –∑–∞ –¥–∞ –º–æ–∂–µ –¥–∞ –Ω–∞–º–∏—Ä–∞–º–µ –ø–æ-—Ç–æ—á–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        """
        if self.verbose:
            print("üìö –°–¢–™–ü–ö–ê 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∏")
            print("-" * 40)
            print(f"–ó–∞—Ä–µ–∂–¥–∞–º–µ {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–∞...")

        # –¢–µ–∫—Å—Ç —Å–ø–ª–∏—Ç—ä—Ä - —Ä–∞–∑–¥–µ–ª—è –≥–æ–ª–µ–º–∏ —Ç–µ–∫—Å—Ç–æ–≤–µ –Ω–∞ –ø–æ-–º–∞–ª–∫–∏ —á–∞—Å—Ç–∏
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,  # –†–∞–∑–º–µ—Ä –Ω–∞ –≤—Å–µ–∫–∏ chunk
            chunk_overlap=50,  # –ü—Ä–∏–ø–æ–∫—Ä–∏–≤–∞–Ω–µ –º–µ–∂–¥—É chunks
            length_function=len,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )

        self.documents = texts
        self.document_chunks = []

        for doc_id, text in enumerate(texts):
            # –†–∞–∑–¥–µ–ª—è–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ chunks
            chunks = text_splitter.split_text(text)

            for chunk_id, chunk in enumerate(chunks):
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "doc_id": doc_id,
                        "chunk_id": chunk_id,
                        "source": f"document_{doc_id}",
                        "total_chunks": len(chunks)
                    }
                )
                self.document_chunks.append(doc)

        if self.verbose:
            print(f"‚úÖ –°—ä–∑–¥–∞–¥–µ–Ω–∏ {len(self.document_chunks)} chunks –æ—Ç {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–∞")
            print("–ü—Ä–∏–º–µ—Ä–µ–Ω chunk:")
            print(f"  Content: {self.document_chunks[0].page_content[:100]}...")
            print(f"  Metadata: {self.document_chunks[0].metadata}")
            print()

    def step3_create_vectorstore(self):
        """
        –°–¢–™–ü–ö–ê 3: –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ vector database

        –¢—É–∫ –ø—Ä–µ–≤—Ä—ä—â–∞–º–µ –≤—Å–∏—á–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤–µ –≤ –≤–µ–∫—Ç–æ—Ä–∏ –∏ –≥–∏ —Å—ä—Ö—Ä–∞–Ω—è–≤–∞–º–µ
        –≤ ChromaDB –∑–∞ –±—ä—Ä–∑–∞ —Ç—ä—Ä—Å–µ–Ω–µ
        """
        if self.verbose:
            print("üóÑÔ∏è –°–¢–™–ü–ö–ê 3: –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ Vector Database")
            print("-" * 40)
            print("–ü—Ä–µ–≤—Ä—ä—â–∞–º–µ –≤—Å–∏—á–∫–∏ chunks –≤ embedding –≤–µ–∫—Ç–æ—Ä–∏...")

        # –°—ä–∑–¥–∞–≤–∞–º–µ ChromaDB vectorstore
        self.vectorstore = Chroma.from_documents(
            documents=self.document_chunks,
            embedding=self.embeddings_model,
            persist_directory=None  # –ù–µ –∑–∞–ø–∏—Å–≤–∞–º–µ –Ω–∞ –¥–∏—Å–∫–∞
        )

        if self.verbose:
            print("‚úÖ Vector database —Å—ä–∑–¥–∞–¥–µ–Ω–∞!")
            print(f"–°—ä—Ö—Ä–∞–Ω–µ–Ω–∏ {len(self.document_chunks)} embedding –≤–µ–∫—Ç–æ—Ä–∞")
            print()

    def step4_similarity_search_explained(self, query: str, k: int = 3):
        """
        –°–¢–™–ü–ö–ê 4: Similarity Search —Å –æ–±—è—Å–Ω–µ–Ω–∏–µ

        –ü–æ–∫–∞–∑–≤–∞ –∫–∞–∫ —Ä–∞–±–æ—Ç–∏ —Ç—ä—Ä—Å–µ–Ω–µ—Ç–æ –Ω–∞ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å—Ç—ä–ø–∫–∞ –ø–æ —Å—Ç—ä–ø–∫–∞
        """
        if self.verbose:
            print("üîç –°–¢–™–ü–ö–ê 4: Similarity Search")
            print("-" * 40)
            print(f"–¢—ä—Ä—Å–∏–º –∑–∞: '{query}'")
            print()

        # –ò–∑–≤—ä—Ä—à–≤–∞–º–µ similarity search
        results = self.vectorstore.similarity_search_with_score(query, k=k)

        if self.verbose:
            print("–ü—Ä–æ—Ü–µ—Å –Ω–∞ —Ç—ä—Ä—Å–µ–Ω–µ:")
            print("1. –ü—Ä–µ–æ–±—Ä–∞–∑—É–≤–∞–º–µ query-—Ç–æ –≤ embedding –≤–µ–∫—Ç–æ—Ä")
            print("2. –°—Ä–∞–≤–Ω—è–≤–∞–º–µ —Å –≤—Å–∏—á–∫–∏ —Å—ä—Ö—Ä–∞–Ω–µ–Ω–∏ –≤–µ–∫—Ç–æ—Ä–∏")
            print("3. –í—Ä—ä—â–∞–º–µ –Ω–∞–π-—Å—Ö–æ–¥–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏")
            print()

            print("–†–µ–∑—É–ª—Ç–∞—Ç–∏:")
            for i, (doc, score) in enumerate(results, 1):
                print(f"{i}. Score: {score:.4f}")
                print(f"   Source: {doc.metadata['source']}")
                print(f"   Content: {doc.page_content[:100]}...")
                print()

        return [doc for doc, score in results]

    def step5_demonstrate_embeddings(self, query: str):
        """
        –°–¢–™–ü–ö–ê 5: –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ embeddings

        –ü–æ–∫–∞–∑–≤–∞ –∫–∞–∫ –∏–∑–≥–ª–µ–∂–¥–∞—Ç –≤–µ–∫—Ç–æ—Ä–∏—Ç–µ –∏ –∫–∞–∫ —Å–µ –∏–∑—á–∏—Å–ª—è–≤–∞ —Å—Ö–æ–¥—Å—Ç–≤–æ—Ç–æ
        """
        if self.verbose:
            print("üßÆ –°–¢–™–ü–ö–ê 5: –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ Embeddings")
            print("-" * 40)

        # –°—ä–∑–¥–∞–≤–∞–º–µ embedding –Ω–∞ query-—Ç–æ
        query_embedding = self.embeddings_model.embed_query(query)

        if self.verbose:
            print(f"Query: '{query}'")
            print(f"Embedding —Ä–∞–∑–º–µ—Ä: {len(query_embedding)} —á–∏—Å–ª–∞")
            print(f"–ü—ä—Ä–≤–∏—Ç–µ 5 —á–∏—Å–ª–∞: {query_embedding[:5]}")
            print()

            # –ü–æ–∫–∞–∑–≤–∞–º–µ –∫–∞–∫ —Å–µ –∏–∑—á–∏—Å–ª—è–≤–∞ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –ø—ä—Ä–≤–∏—è document
            if self.document_chunks:
                first_doc_embedding = self.embeddings_model.embed_query(
                    self.document_chunks[0].page_content
                )

                # Cosine similarity
                similarity = cosine_similarity(
                    [query_embedding],
                    [first_doc_embedding]
                )[0][0]

                print(f"–°—Ö–æ–¥—Å—Ç–≤–æ —Å –ø—ä—Ä–≤–∏—è document: {similarity:.4f}")
                print(f"Document: {self.document_chunks[0].page_content[:100]}...")
                print()

    def step6_context_augmentation(self, query: str, relevant_docs: List[Document]):
        """
        –°–¢–™–ü–ö–ê 6: Context Augmentation

        –ü–æ–∫–∞–∑–≤–∞ –∫–∞–∫ —Å–µ –∫–æ–º–±–∏–Ω–∏—Ä–∞ query-—Ç–æ —Å –Ω–∞–º–µ—Ä–µ–Ω–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∏
        """
        if self.verbose:
            print("üîó –°–¢–™–ü–ö–ê 6: Context Augmentation")
            print("-" * 40)

        # –°—ä–∑–¥–∞–≤–∞–º–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç –Ω–∞–º–µ—Ä–µ–Ω–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∏
        context = "\n\n".join([doc.page_content for doc in relevant_docs])

        # –°—ä–∑–¥–∞–≤–∞–º–µ augmented prompt
        augmented_prompt = f"""
–ö–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∏:
{context}

–í—ä–ø—Ä–æ—Å: {query}

–ú–æ–ª—è, –æ—Ç–≥–æ–≤–æ—Ä–∏ –Ω–∞ –≤—ä–ø—Ä–æ—Å–∞ –≤—ä–∑ –æ—Å–Ω–æ–≤–∞ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç.
"""

        if self.verbose:
            print("–ö–æ–º–±–∏–Ω–∏—Ä–∞–Ω–µ –Ω–∞ query + context:")
            print(f"–û—Ä–∏–≥–∏–Ω–∞–ª–µ–Ω –≤—ä–ø—Ä–æ—Å: {query}")
            print(f"–î–æ–±–∞–≤–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç {len(relevant_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–∞")
            print(f"–û–±—â —Ä–∞–∑–º–µ—Ä –Ω–∞ prompt: {len(augmented_prompt)} —Å–∏–º–≤–æ–ª–∞")
            print()
            print("Augmented Prompt:")
            print(augmented_prompt[:300] + "..." if len(augmented_prompt) > 300 else augmented_prompt)
            print()

        return augmented_prompt

    def step7_generate_response(self, augmented_prompt: str):
        """
        –°–¢–™–ü–ö–ê 7: Response Generation (—Å–∏–º—É–ª–∞—Ü–∏—è)

        –í —Ä–µ–∞–ª–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ —Ç—É–∫ –±–∏ —Å–µ –∏–∑–ø–æ–ª–∑–≤–∞–ª LLM –∫–∞—Ç–æ GPT, Claude, etc.
        –ó–∞ –¥–µ–º–æ —Ü–µ–ª–∏—Ç–µ —â–µ —Å–∏–º—É–ª–∏—Ä–∞–º–µ –æ—Ç–≥–æ–≤–æ—Ä
        """
        if self.verbose:
            print("ü§ñ –°–¢–™–ü–ö–ê 7: Response Generation")
            print("-" * 40)
            print("–í —Ä–µ–∞–ª–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ —Ç—É–∫ –±–∏ —Å–µ –∏–∑–ø–æ–ª–∑–≤–∞–ª LLM (GPT, Claude, Llama, etc.)")
            print("–ó–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —â–µ —Å–∏–º—É–ª–∏—Ä–∞–º–µ '—É–º–µ–Ω' –æ—Ç–≥–æ–≤–æ—Ä")
            print()

        # –°–∏–º—É–ª–∏—Ä–∞–Ω –æ—Ç–≥–æ–≤–æ—Ä
        doc_count = len(augmented_prompt.split('–í—ä–ø—Ä–æ—Å:')[0].split('\n\n'))
        response = f"""
                    [–°–ò–ú–£–õ–ò–†–ê–ù LLM –û–¢–ì–û–í–û–†]
                    
                    –í—ä–∑ –æ—Å–Ω–æ–≤–∞ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–µ–Ω–∞—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –º–æ–≥–∞ –¥–∞ –æ—Ç–≥–æ–≤–æ—Ä—è –Ω–∞ –≤—ä–ø—Ä–æ—Å–∞.
                    
                    –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è—Ç–∞ –µ –≤–∑–µ—Ç–∞ –æ—Ç {doc_count} –¥–æ–∫—É–º–µ–Ω—Ç–∞ 
                    –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏ –¥–µ—Ç–∞–π–ª–∏.
                    
                    (–í —Ä–µ–∞–ª–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ —Ç—É–∫ –±–∏ —Å–µ –≥–µ–Ω–µ—Ä–∏—Ä–∞–ª —Ç–æ—á–µ–Ω –æ—Ç–≥–æ–≤–æ—Ä –æ—Ç LLM –º–æ–¥–µ–ª)
                    """

        if self.verbose:
            print("–ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω –æ—Ç–≥–æ–≤–æ—Ä:")
            print(response)
            print()

        return response

    def full_rag_pipeline(self, query: str):
        """
        –ü—ä–ª–µ–Ω RAG pipeline —Å—ä—Å –≤—Å–∏—á–∫–∏ —Å—Ç—ä–ø–∫–∏
        """
        print("üéØ –ü–™–õ–ï–ù RAG PIPELINE")
        print("=" * 60)
        print(f"Query: '{query}'")
        print("=" * 60)
        print()

        # –°—Ç—ä–ø–∫–∞ –ø–æ —Å—Ç—ä–ø–∫–∞
        relevant_docs = self.step4_similarity_search_explained(query)
        self.step5_demonstrate_embeddings(query)
        augmented_prompt = self.step6_context_augmentation(query, relevant_docs)
        response = self.step7_generate_response(augmented_prompt)

        return {
            "query": query,
            "relevant_documents": [doc.page_content for doc in relevant_docs],
            "augmented_prompt": augmented_prompt,
            "response": response
        }


def interactive_learning_demo():
    """
    –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ –¥–µ–º–æ –∑–∞ —É—á–µ–Ω–µ –Ω–∞ RAG –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
    """
    print("üéì –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–û RAG –î–ï–ú–û –ó–ê –£–ß–ï–ù–ï")
    print("=" * 60)
    print("–¢–æ–≤–∞ –¥–µ–º–æ —â–µ —Ç–∏ –ø–æ–∫–∞–∂–µ –∫–∞–∫ —Ä–∞–±–æ—Ç–∏ RAG —Å—Ç—ä–ø–∫–∞ –ø–æ —Å—Ç—ä–ø–∫–∞")
    print("–í—Å—è–∫–∞ —Å—Ç—ä–ø–∫–∞ —â–µ –±—ä–¥–µ –æ–±—è—Å–Ω–µ–Ω–∞ –ø–æ–¥—Ä–æ–±–Ω–æ")
    print("=" * 60)
    print()

    # –°—ä–∑–¥–∞–≤–∞–º–µ RAG —Å–∏—Å—Ç–µ–º–∞
    rag = InteractiveRAGSystem(verbose=True)

    # –ü—Ä–∏–º–µ—Ä–Ω–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –∑–∞ AI/ML –∏–Ω—Ç–µ—Ä–≤—é
    sample_documents = [
        """
        Retrieval-Augmented Generation (RAG) –µ —Ç–µ—Ö–Ω–∏–∫–∞ –∑–∞ –ø–æ–¥–æ–±—Ä—è–≤–∞–Ω–µ –Ω–∞ Large Language Models.
        RAG —Ä–∞–±–æ—Ç–∏ –≤ —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω–∏ —Å—Ç—ä–ø–∫–∏: –ø—ä—Ä–≤–æ —Ç—ä—Ä—Å–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –≤ –±–∞–∑–∞ –¥–∞–Ω–Ω–∏,
        –ø–æ—Å–ª–µ –¥–æ–±–∞–≤—è —Ç–µ–∑–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –∫–∞—Ç–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫—ä–º –≤—ä–ø—Ä–æ—Å–∞, –∏ –Ω–∞–∫—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä–∞ –æ—Ç–≥–æ–≤–æ—Ä
        –≤—ä–∑ –æ—Å–Ω–æ–≤–∞ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–µ–Ω–∞—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è. –¢–æ–≤–∞ –ø–æ–∑–≤–æ–ª—è–≤–∞ –Ω–∞ –º–æ–¥–µ–ª–∞ –¥–∞ –∏–∑–ø–æ–ª–∑–≤–∞ 
        –∞–∫—Ç—É–∞–ª–Ω–∞ –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –∫–æ—è—Ç–æ –Ω–µ –µ –±–∏–ª–∞ —á–∞—Å—Ç –æ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤—ä—á–Ω–∏—Ç–µ –º—É –¥–∞–Ω–Ω–∏.
        """,

        """
        Vector databases —Å–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ –±–∞–∑–∏ –¥–∞–Ω–Ω–∏ –∑–∞ —Å—ä—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞ embedding –≤–µ–∫—Ç–æ—Ä–∏.
        –¢–µ –ø–æ–∑–≤–æ–ª—è–≤–∞—Ç –±—ä—Ä–∑–æ —Ç—ä—Ä—Å–µ–Ω–µ –Ω–∞ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∏—Ç–µ —á—Ä–µ–∑ –∞–ª–≥–æ—Ä–∏—Ç–º–∏ –∫–∞—Ç–æ
        cosine similarity, dot product –∏–ª–∏ euclidean distance. –ü–æ–ø—É–ª—è—Ä–Ω–∏ vector databases
        –≤–∫–ª—é—á–≤–∞—Ç ChromaDB, Pinecone, Weaviate –∏ Milvus. –¢–µ–∑–∏ –±–∞–∑–∏ –¥–∞–Ω–Ω–∏ —Å–∞ –∫–ª—é—á–æ–≤–∏ –∑–∞
        RAG —Å–∏—Å—Ç–µ–º–∏, –∑–∞—â–æ—Ç–æ –ø–æ–∑–≤–æ–ª—è–≤–∞—Ç –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –Ω–∞–º–∏—Ä–∞–Ω–µ –Ω–∞ –Ω–∞–π-—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∏.
        """,

        """
        LangChain –µ framework –∑–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å Large Language Models.
        –¢–æ–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—è –≥–æ—Ç–æ–≤–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –∑–∞ —Å–æ–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ chains, agents –∏ tools.
        LangChain –ø–æ–¥–¥—ä—Ä–∂–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ LLM –¥–æ—Å—Ç–∞–≤—á–∏—Ü–∏ –∫–∞—Ç–æ OpenAI, Anthropic, Hugging Face.
        –û—Å–Ω–æ–≤–Ω–∏—Ç–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –≤–∫–ª—é—á–≤–∞—Ç: Prompts, Chains, Agents, Memory, Tools –∏ Callbacks.
        –¢–æ–≤–∞ –ø—Ä–∞–≤–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞—Ç–∞ –Ω–∞ AI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –ø–æ-–ª–µ—Å–Ω–∞ –∏ –ø–æ-—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–∞–Ω–∞.
        """,

        """
        Embeddings —Å–∞ —á–∏—Å–ª–æ–≤–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–∞ —Ç–µ–∫—Å—Ç, –∫–æ–∏—Ç–æ —É–ª–∞–≤—è—Ç —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ.
        –¢–µ —Å–µ —Å—ä–∑–¥–∞–≤–∞—Ç –æ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ –º–æ–¥–µ–ª–∏ –∫–∞—Ç–æ sentence-transformers. –ö–∞—á–µ—Å—Ç–≤–µ–Ω–∏
        embeddings –ø–æ–∑–≤–æ–ª—è–≤–∞—Ç –Ω–∞ –∫–æ–º–ø—é—Ç—ä—Ä–∞ –¥–∞ —Ä–∞–∑–±–∏—Ä–∞, —á–µ "–∫–æ–ª–∞" –∏ "–∞–≤—Ç–æ–º–æ–±–∏–ª" —Å–∞ —Å—Ö–æ–¥–Ω–∏,
        –≤—ä–ø—Ä–µ–∫–∏ —á–µ —Å–∞ —Ä–∞–∑–ª–∏—á–Ω–∏ –¥—É–º–∏. –ó–∞ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ embeddings —Å–µ –∏–∑–ø–æ–ª–∑–≤–∞—Ç –º–æ–¥–µ–ª–∏ –∫–∞—Ç–æ
        BERT, Sentence-BERT, OpenAI embeddings –∏–ª–∏ Hugging Face transformers.
        """,

        """
        Prompt Engineering –µ –∏–∑–∫—É—Å—Ç–≤–æ—Ç–æ –Ω–∞ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–∏ prompts –∑–∞ LLM –º–æ–¥–µ–ª–∏.
        –¢–µ—Ö–Ω–∏–∫–∏ –≤–∫–ª—é—á–≤–∞—Ç few-shot learning (–¥–∞–≤–∞–Ω–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∏), chain-of-thought reasoning
        (—Å—Ç—ä–ø–∫–∞ –ø–æ —Å—Ç—ä–ø–∫–∞ –º–∏—Å–ª–µ–Ω–µ), –∏ role-based prompting (–∑–∞–¥–∞–≤–∞–Ω–µ –Ω–∞ —Ä–æ–ª—è).
        –î–æ–±—Ä–∏—Ç–µ prompts –º–æ–≥–∞—Ç –∑–Ω–∞—á–∏—Ç–µ–ª–Ω–æ –¥–∞ –ø–æ–¥–æ–±—Ä—è—Ç –∫–∞—á–µ—Å—Ç–≤–æ—Ç–æ –Ω–∞ –æ—Ç–≥–æ–≤–æ—Ä–∏—Ç–µ –æ—Ç LLM.
        Prompt engineering –µ –∫—Ä–∏—Ç–∏—á–Ω–∞ —É–º–µ–Ω–∏—è –∑–∞ —Ä–∞–±–æ—Ç–∞ —Å AI –º–æ–¥–µ–ª–∏ –≤ –ø—Ä–æ–¥—É–∫—Ü–∏—è.
        """
    ]

    # –°—Ç—ä–ø–∫–∞ –ø–æ —Å—Ç—ä–ø–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
    rag.step1_setup_embeddings()
    rag.step2_load_documents(sample_documents)
    rag.step3_create_vectorstore()

    # –ü—Ä–∏–º–µ—Ä–Ω–∏ –≤—ä–ø—Ä–æ—Å–∏ –∑–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
    demo_questions = [
        "–ö–∞–∫ —Ä–∞–±–æ—Ç–∏ RAG?",
        "–ö–∞–∫–≤–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–≤–∞—Ç vector databases?",
        "–ö–∞–∫–≤–∏ —Å–∞ –æ—Å–Ω–æ–≤–Ω–∏—Ç–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –Ω–∞ LangChain?",
        "–ö–∞–∫–≤–æ —Å–∞ embeddings?",
        "–ö–∞–∫–≤–∏ —Ç–µ—Ö–Ω–∏–∫–∏ —Å–µ –∏–∑–ø–æ–ª–∑–≤–∞—Ç –≤ prompt engineering?"
    ]

    print("üéØ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –° –ü–†–ò–ú–ï–†–ù–ò –í–™–ü–†–û–°–ò")
    print("=" * 60)

    for i, question in enumerate(demo_questions, 1):
        print(f"\nüìù –î–ï–ú–û –í–™–ü–†–û–° {i}: {question}")
        print("-" * 80)

        # –ü–æ–∫–∞–∑–≤–∞–º–µ —Å–∞–º–æ retrieval —á–∞—Å—Ç—Ç–∞ –∑–∞ –∫—Ä–∞—Ç–∫–æ—Å—Ç
        relevant_docs = rag.step4_similarity_search_explained(question, k=2)

        # –ú–∞–ª–∫–∞ –ø–∞—É–∑–∞ –∑–∞ —á–µ—Ç–µ–Ω–µ
        input("–ù–∞—Ç–∏—Å–Ω–∏ Enter –∑–∞ —Å–ª–µ–¥–≤–∞—â–∏—è –≤—ä–ø—Ä–æ—Å...")

    print("\nüéØ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–ê –ß–ê–°–¢")
    print("=" * 60)
    print("–°–µ–≥–∞ –º–æ–∂–µ—à –¥–∞ –∑–∞–¥–∞–≤–∞—à —Å–≤–æ–∏ –≤—ä–ø—Ä–æ—Å–∏!")
    print("–ù–∞–ø–∏—à–∏ 'exit' –∑–∞ –∏–∑—Ö–æ–¥")
    print()

    while True:
        user_question = input("‚ùì –¢–≤–æ—è—Ç –≤—ä–ø—Ä–æ—Å: ").strip()

        if user_question.lower() in ['exit', 'quit', '–∏–∑—Ö–æ–¥']:
            print("üëã –£—Å–ø–µ—Ö!")
            break

        if not user_question:
            continue

        try:
            result = rag.full_rag_pipeline(user_question)
            print("\n" + "=" * 80)
            input("–ù–∞—Ç–∏—Å–Ω–∏ Enter –∑–∞ —Å–ª–µ–¥–≤–∞—â –≤—ä–ø—Ä–æ—Å...")
        except Exception as e:
            print(f"‚ùå –ì—Ä–µ—à–∫–∞: {e}")


if __name__ == "__main__":
    # –°—Ç–∞—Ä—Ç–∏—Ä–∞–º–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Ç–æ –¥–µ–º–æ
    interactive_learning_demo()
